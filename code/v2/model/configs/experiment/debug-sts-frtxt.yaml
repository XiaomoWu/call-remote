# @package _global_
defaults:
  - override /datamodule: sts
  - override /model: frtxt
  - override /scheduler: null
  - override /callbacks:
    - model_checkpoint
    - wandb
  - override /trainer: default
  - override /logger: wandb

  # key args!
  - override /weighting_method: fixed
  - override /optimizer: adamw
  - override /strategy: ddp-sharded  # "deepspeed" or "ddp"
  
seed: 42
mode: train
model_id: debug
yqtr: '2019'
print_config: false
test_after_train: false

datamodule:
  bsz: 64
  eff_bsz: 256
  val_bsz: 256
  tasks: [Title]
  num_workers: 4

model:
  pretrained_sent_encoder_name: distilbert-base-uncased
  hidden_size: 50
  dropout: 0.3
  pre_classifier_activation: relu  # "relu" or "tanh"
  unfreeze_embedding: true
  n_unfreezed_layers: all
  summarize_method: cls
  fc_lr: 5e-5
  sent_encoder_lr: 5e-5
  doc_pooling_method: avgpool

  use_finratios: false
  use_mantxts: false

  tokenizer_cfg:
    max_length: 512
    padding: true
    truncation: true
    return_overflowing_tokens: true
    return_tensors: pt
    pad_to_multiple_of: 8
  
optimizer:
  lr: 1

weighting_method:
  Fixed:
    init_task_weights: [1.0]

  # GradPerp:
  #   normalize_G: false
  #   normalize_R: false
  #   G_avg_mode: EMA
  #   qr_mode: cos
  #   M: 1
  #   beta: 0.9

trainer:
  devices: [0, 1]
  min_epochs: 4
  max_epochs: 4
  num_sanity_val_steps: 0
  stochastic_weight_avg: false
  precision: 16
  val_check_interval: 1.0
  limit_train_batches: 0.1
  limit_val_batches: 1.0
  detect_anomaly: true

plugins:
  stage: 2
  offload_optimizer: true
  offload_optimizer_device: cpu
  offload_parameters: true
  offload_params_device: cpu
  cpu_checkpointing: false
  partition_activations: false
  logging_batch_size_per_gpu: 1
  logging_level: 40  # 40: ERROR, 30: WARNING 

callbacks:
  model_checkpoint:
    save_top_k: 0

logger:
  log_model: false
  offline: false
